
'''Scripts for evaluating the chatbot's performance.'''
from rag_pipeline import rag
from evaluation import evaluation_matrics
import time


def load_golden_examples():
    '''
    Below golden questions and answers are generated by chatGPT by providing the publication.
    Ideally, the golden examples should be provided by Business and can be stored in persistent storage for evaluation. 
    For this prototype, I am manually storing them in pandas df.
    '''
    data = {
        "questions": [
            "What is the HER-2/neu oncogene?",
            "How does HER-2/neu amplification affect breast cancer prognosis?",
            "What percentage of breast cancer cases have HER-2/neu amplification?",
            "Why is HER-2/neu amplification a critical biomarker?",
            "Does HER-2/neu amplification impact the effectiveness of chemotherapy?",
            "What other factors are used to predict breast cancer prognosis?",
            "Can HER-2/neu amplification be detected in early-stage cancer?",
            "How does the study contribute to treatment strategies for breast cancer?",
            "What is the survival rate for patients with HER-2/neu amplification?",
            "How can HER-2/neu amplification impact treatment decisions?"
        ],
        "golden_responses": [
            "The HER-2/neu oncogene encodes a protein involved in cell growth and division. Its amplification is linked to aggressive breast cancer forms.",
            "Amplification of HER-2/neu is associated with a higher risk of relapse and worse survival outcomes in breast cancer patients.",
            "Approximately 30% of breast cancer cases exhibit HER-2/neu amplification.",
            "It helps predict the likelihood of relapse and survival, making it crucial for treatment decisions.",
            "Yes, tumors with HER-2/neu amplification are often more resistant to conventional chemotherapy but can respond to targeted therapies.",
            "Hormone receptor status, tumor size, lymph node involvement, and HER-2/neu amplification are key factors.",
            "Yes, it can be detected through tests like fluorescence in situ hybridization (FISH) or immunohistochemistry (IHC).",
            "It emphasizes the importance of HER-2/neu testing for personalized treatment, particularly the use of targeted therapies like trastuzumab.",
            "Patients with HER-2/neu amplification generally have a lower survival rate compared to those without it.",
            "Patients with HER-2/neu amplification may benefit from HER-2-targeted therapies, influencing treatment protocols."
        ]
    }
    golden_examples = pd.DataFrame(data)
    print(golden_examples)
    return golden_examples

def run_test_set(test_queries):
    """
    Run a diverse set of test queries and evaluate the chatbot's responses.
    """
    rag_obj = rag.RAG()
    responses = []
    response_time = []
    contexts = []
    for query in test_queries:
        start_time = time.time()
        context, response = rag_obj.generate_response_for_evaluation(query)
        context = context.split("\n")
        responses.append(response)
        end_time = time.time()
        contexts.append(context)
        response_time.append(evaluation_matrics.measure_response_time(start_time, end_time))
    return contexts, responses, response_time


def main():
    ''' This will run the entire testing & evaluation pipeline'''
    testing_dataset = load_golden_examples()
    testing_dataset['generated_contexts'] , testing_dataset['generated_responses'], testing_dataset['response_time']  = run_test_set(testing_dataset['questions'])
    
    # Evaluate the offline metrics
    generated_responses = testing_dataset['generated_responses'].tolist()
    golden_responses = testing_dataset['golden_responses'].tolist()
    testing_dataset['bleu_score'] = evaluation_matrics.evaluate_bleu(generated_responses, golden_responses)
    testing_dataset['rouge_score'] = evaluation_matrics.evaluate_rouge(generated_responses, golden_responses)

    # Calculate RAGA score. I commented the code as I was getting some errors.
    testing_dataset['generated_contexts'] = testing_dataset['generated_contexts'].apply(lambda x: x.split("\n"))
    questions = testing_dataset['questions'].tolist()
    generated_contexts = testing_dataset['generated_contexts'].tolist()
    #testing_dataset['golden_responses'] = testing_dataset['golden_responses'].apply(lambda x: [x])
    #raga_df = evaluation_matrics.evaluate_raga(questions, generated_responses, generated_contexts, golden_responses)
    #evaluation_df = pd.concat([testing_dataset, raga_df], axis=1)
    
    # Evaluate the Business (Online) metrics
    accuracy = evaluation_matrics.evaluate_accuracy(generated_responses, golden_responses)
    satisfaction_score = evaluation_matrics.calculate_user_satisfaction_score()
    avg_response_time = testing_dataset['response_time'].mean()

    #This summary report can be used to compare with benchmark for deploying.
    print(f"Evaluation Summary:")
    print(f"ROUGE score: {testing_dataset['rouge_score']}")
    print(f"BLEU score: {testing_dataset['bleu_score'].mean()}")
    print(f"Accuracy: {accuracy}")
    print(f"Customer Satisfaction Score: {satisfaction_score}%")
    print(f"Average Response Time: {avg_response_time}%")

    # The row-wise evaluation score is stored in csv for further review.
    evaluation_df = testing_dataset.copy()
    evaluation_df.to_csv('evaluation/testing_evaluation.csv')

if __name__ == "__main__":
    main()